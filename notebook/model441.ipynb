{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow.keras as keras\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler\n",
    "DATA_PATH = \"data441_1.json\"\n",
    "\n",
    "\n",
    "def load_data(data_path):\n",
    "    \"\"\"Loads training dataset from json file.\n",
    "        :param data_path (str): Path to json file containing data\n",
    "        :return X (ndarray): Inputs\n",
    "        :return y (ndarray): Targets\n",
    "    \"\"\"\n",
    "\n",
    "    with open(data_path, \"r\") as fp:\n",
    "        data = json.load(fp)\n",
    "\n",
    "    X = np.array(data[\"mfcc\"])\n",
    "    y = np.array(data[\"labels\"])\n",
    "    # Encode Labels\n",
    "    labelencoder = LabelEncoder()\n",
    "    labelencoder.fit(y)\n",
    "    classes_num = labelencoder.transform(y)\n",
    "\n",
    "    #OneHotEncoding\n",
    "    encoder=OneHotEncoder(sparse=False, categories=\"auto\")\n",
    "    y=encoder.fit_transform(classes_num.reshape(len(classes_num),1))\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def plot_history(history):\n",
    "    \"\"\"Plots accuracy/loss for training/validation set as a function of the epochs\n",
    "        :param history: Training history of model\n",
    "        :return:\n",
    "    \"\"\"\n",
    "\n",
    "    fig, axs = plt.subplots(2)\n",
    "\n",
    "    # create accuracy sublpot\n",
    "    axs[0].plot(history.history[\"accuracy\"], label=\"train accuracy\")\n",
    "    axs[0].plot(history.history[\"val_accuracy\"], label=\"test accuracy\")\n",
    "    axs[0].set_ylabel(\"Accuracy\")\n",
    "    axs[0].legend(loc=\"lower right\")\n",
    "    axs[0].set_title(\"Accuracy eval\")\n",
    "\n",
    "    # create error sublpot\n",
    "    axs[1].plot(history.history[\"loss\"], label=\"train error\")\n",
    "    axs[1].plot(history.history[\"val_loss\"], label=\"test error\")\n",
    "    axs[1].set_ylabel(\"Error\")\n",
    "    axs[1].set_xlabel(\"Epoch\")\n",
    "    axs[1].legend(loc=\"upper right\")\n",
    "    axs[1].set_title(\"Error eval\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def prepare_datasets(test_size, validation_size):\n",
    "    \"\"\"Loads data and splits it into train, validation and test sets.\n",
    "    :param test_size (float): Value in [0, 1] indicating percentage of data set to allocate to test split\n",
    "    :param validation_size (float): Value in [0, 1] indicating percentage of train set to allocate to validation split\n",
    "    :return X_train (ndarray): Input training set\n",
    "    :return X_validation (ndarray): Input validation set\n",
    "    :return X_test (ndarray): Input test set\n",
    "    :return y_train (ndarray): Target training set\n",
    "    :return y_validation (ndarray): Target validation set\n",
    "    :return y_test (ndarray): Target test set\n",
    "    \"\"\"\n",
    "\n",
    "    # load data\n",
    "    X, y = load_data(DATA_PATH)\n",
    "\n",
    "    # create train, validation and test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size)\n",
    "    X_train, X_validation, y_train, y_validation = train_test_split(X_train, y_train, test_size=validation_size)\n",
    "\n",
    "    return X_train, X_validation, X_test, y_train, y_validation, y_test\n",
    "\n",
    "\n",
    "def build_model(input_shape):\n",
    "    \"\"\"Generates RNN-LSTM model\n",
    "    :param input_shape (tuple): Shape of input set\n",
    "    :return model: RNN-LSTM model\n",
    "    \"\"\"\n",
    "\n",
    "    # build network topology\n",
    "    model = keras.Sequential()\n",
    "\n",
    "    # 2 LSTM layers\n",
    "    model.add(keras.layers.LSTM(256, input_shape=input_shape, return_sequences=True))\n",
    "    model.add(keras.layers.Dropout(0.3))\n",
    "    model.add(keras.layers.LSTM(256, return_sequences=True))\n",
    "    model.add(keras.layers.Dropout(0.3))\n",
    "    \n",
    "    model.add(keras.layers.LSTM(256))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    model.add(keras.layers.Dropout(0.3))\n",
    "    # dense layer\n",
    "    model.add(keras.layers.Dense(256, activation='relu'))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    model.add(keras.layers.Dropout(0.3)) \n",
    "\n",
    "    # output layer\n",
    "    model.add(keras.layers.Dense(11, activation='sigmoid'))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_validation, X_test, y_train, y_validation, y_test = prepare_datasets(0.1, 0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (X_train.shape[1], X_train.shape[2]) # 130, 13\n",
    "model = build_model(input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(5128, 259, 13)"
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop',\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"./callback441/weights-improvementx2-{epoch:02d}-{loss:.4f}-bigger.hdf5\"    \n",
    "checkpoint = keras.callbacks.ModelCheckpoint(\n",
    "    filepath, monitor='loss', \n",
    "    verbose=0,        \n",
    "    save_best_only=True,        \n",
    "    mode='min'\n",
    ")    \n",
    "callbacks_list = [checkpoint]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "===] - 51s 10ms/sample - loss: 0.0277 - accuracy: 0.9905 - val_loss: 0.3528 - val_accuracy: 0.9209\nEpoch 69/200\n5128/5128 [==============================] - 51s 10ms/sample - loss: 0.0294 - accuracy: 0.9907 - val_loss: 0.3984 - val_accuracy: 0.9202\nEpoch 70/200\n5128/5128 [==============================] - 51s 10ms/sample - loss: 0.0278 - accuracy: 0.9905 - val_loss: 0.3535 - val_accuracy: 0.9271\nEpoch 71/200\n5128/5128 [==============================] - 51s 10ms/sample - loss: 0.0253 - accuracy: 0.9910 - val_loss: 0.3160 - val_accuracy: 0.9311\nEpoch 72/200\n5128/5128 [==============================] - 51s 10ms/sample - loss: 0.0243 - accuracy: 0.9917 - val_loss: 0.4699 - val_accuracy: 0.9129\nEpoch 73/200\n5128/5128 [==============================] - 51s 10ms/sample - loss: 0.0295 - accuracy: 0.9904 - val_loss: 0.3174 - val_accuracy: 0.9263\nEpoch 74/200\n5128/5128 [==============================] - 51s 10ms/sample - loss: 0.0264 - accuracy: 0.9909 - val_loss: 0.3403 - val_accuracy: 0.9225\nEpoch 75/200\n5128/5128 [==============================] - 51s 10ms/sample - loss: 0.0249 - accuracy: 0.9913 - val_loss: 0.4131 - val_accuracy: 0.9146\nEpoch 76/200\n5128/5128 [==============================] - 51s 10ms/sample - loss: 0.0247 - accuracy: 0.9917 - val_loss: 0.4301 - val_accuracy: 0.9149\nEpoch 77/200\n5128/5128 [==============================] - 51s 10ms/sample - loss: 0.0223 - accuracy: 0.9926 - val_loss: 0.4497 - val_accuracy: 0.9132\nEpoch 78/200\n5128/5128 [==============================] - 51s 10ms/sample - loss: 0.0204 - accuracy: 0.9929 - val_loss: 0.4983 - val_accuracy: 0.9094\nEpoch 79/200\n5128/5128 [==============================] - 51s 10ms/sample - loss: 0.0249 - accuracy: 0.9917 - val_loss: 0.3517 - val_accuracy: 0.9260\nEpoch 80/200\n5128/5128 [==============================] - 51s 10ms/sample - loss: 0.0237 - accuracy: 0.9920 - val_loss: 0.3416 - val_accuracy: 0.9215\nEpoch 81/200\n5128/5128 [==============================] - 51s 10ms/sample - loss: 0.0229 - accuracy: 0.9924 - val_loss: 0.4612 - val_accuracy: 0.9151\nEpoch 82/200\n5128/5128 [==============================] - 51s 10ms/sample - loss: 0.0259 - accuracy: 0.9914 - val_loss: 0.2888 - val_accuracy: 0.9346\nEpoch 83/200\n5128/5128 [==============================] - 51s 10ms/sample - loss: 0.0215 - accuracy: 0.9931 - val_loss: 0.4134 - val_accuracy: 0.9177\nEpoch 84/200\n5128/5128 [==============================] - 51s 10ms/sample - loss: 0.0241 - accuracy: 0.9923 - val_loss: 0.3077 - val_accuracy: 0.9283\nEpoch 85/200\n5128/5128 [==============================] - 51s 10ms/sample - loss: 0.0188 - accuracy: 0.9938 - val_loss: 0.3287 - val_accuracy: 0.9321\nEpoch 86/200\n5128/5128 [==============================] - 51s 10ms/sample - loss: 0.0209 - accuracy: 0.9932 - val_loss: 0.3328 - val_accuracy: 0.9287\nEpoch 87/200\n5128/5128 [==============================] - 51s 10ms/sample - loss: 0.0189 - accuracy: 0.9938 - val_loss: 0.3039 - val_accuracy: 0.9348\nEpoch 88/200\n5128/5128 [==============================] - 51s 10ms/sample - loss: 0.0223 - accuracy: 0.9927 - val_loss: 0.2890 - val_accuracy: 0.9319\nEpoch 89/200\n5128/5128 [==============================] - 51s 10ms/sample - loss: 0.0201 - accuracy: 0.9932 - val_loss: 0.3810 - val_accuracy: 0.9214\nEpoch 90/200\n5128/5128 [==============================] - 51s 10ms/sample - loss: 0.0223 - accuracy: 0.9929 - val_loss: 0.2973 - val_accuracy: 0.9331\nEpoch 91/200\n5128/5128 [==============================] - 51s 10ms/sample - loss: 0.0185 - accuracy: 0.9940 - val_loss: 0.3468 - val_accuracy: 0.9246\nEpoch 92/200\n5128/5128 [==============================] - 51s 10ms/sample - loss: 0.0195 - accuracy: 0.9938 - val_loss: 0.3969 - val_accuracy: 0.9242\nEpoch 93/200\n5128/5128 [==============================] - 51s 10ms/sample - loss: 0.0201 - accuracy: 0.9935 - val_loss: 0.4319 - val_accuracy: 0.9142\nEpoch 94/200\n5128/5128 [==============================] - 51s 10ms/sample - loss: 0.0251 - accuracy: 0.9929 - val_loss: 0.3449 - val_accuracy: 0.9277\nEpoch 95/200\n5128/5128 [==============================] - 51s 10ms/sample - loss: 0.0201 - accuracy: 0.9937 - val_loss: 0.4057 - val_accuracy: 0.9182\nEpoch 96/200\n5128/5128 [==============================] - 51s 10ms/sample - loss: 0.0193 - accuracy: 0.9935 - val_loss: 0.3138 - val_accuracy: 0.9323\nEpoch 97/200\n5128/5128 [==============================] - 51s 10ms/sample - loss: 0.0174 - accuracy: 0.9945 - val_loss: 0.2875 - val_accuracy: 0.9319\nEpoch 98/200\n5128/5128 [==============================] - 51s 10ms/sample - loss: 0.0208 - accuracy: 0.9932 - val_loss: 0.4301 - val_accuracy: 0.9157\nEpoch 99/200\n5128/5128 [==============================] - 51s 10ms/sample - loss: 0.0195 - accuracy: 0.9941 - val_loss: 0.3579 - val_accuracy: 0.9298\nEpoch 100/200\n5128/5128 [==============================] - 51s 10ms/sample - loss: 0.0173 - accuracy: 0.9948 - val_loss: 0.2909 - val_accuracy: 0.9346\nEpoch 101/200\n5128/5128 [==============================] - 51s 10ms/sample - loss: 0.0157 - accuracy: 0.9954 - val_loss: 0.3352 - val_accuracy: 0.9288\nEpoch 102/200\n5128/5128 [==============================] - 51s 10ms/sample - loss: 0.0195 - accuracy: 0.9945 - val_loss: 0.3673 - val_accuracy: 0.9213\nEpoch 103/200\n5128/5128 [==============================] - 51s 10ms/sample - loss: 0.0225 - accuracy: 0.9927 - val_loss: 0.3796 - val_accuracy: 0.9236\nEpoch 104/200\n5128/5128 [==============================] - 52s 10ms/sample - loss: 0.0193 - accuracy: 0.9937 - val_loss: 0.3802 - val_accuracy: 0.9232\nEpoch 105/200\n5128/5128 [==============================] - 51s 10ms/sample - loss: 0.0193 - accuracy: 0.9940 - val_loss: 0.3107 - val_accuracy: 0.9327\nEpoch 106/200\n5128/5128 [==============================] - 51s 10ms/sample - loss: 0.0165 - accuracy: 0.9950 - val_loss: 0.3025 - val_accuracy: 0.9330\nEpoch 107/200\n5128/5128 [==============================] - 51s 10ms/sample - loss: 0.0172 - accuracy: 0.9950 - val_loss: 0.3254 - val_accuracy: 0.9317\nEpoch 108/200\n5128/5128 [==============================] - 51s 10ms/sample - loss: 0.0136 - accuracy: 0.9954 - val_loss: 0.3844 - val_accuracy: 0.9230\nEpoch 109/200\n5128/5128 [==============================] - 51s 10ms/sample - loss: 0.0137 - accuracy: 0.9956 - val_loss: 0.3446 - val_accuracy: 0.9303\nEpoch 110/200\n5128/5128 [==============================] - 51s 10ms/sample - loss: 0.0134 - accuracy: 0.9954 - val_loss: 0.3715 - val_accuracy: 0.9268\nEpoch 111/200\n5128/5128 [==============================] - 51s 10ms/sample - loss: 0.0151 - accuracy: 0.9955 - val_loss: 0.3085 - val_accuracy: 0.9338\nEpoch 112/200\n5128/5128 [==============================] - 51s 10ms/sample - loss: 0.0130 - accuracy: 0.9958 - val_loss: 0.3834 - val_accuracy: 0.9271\nEpoch 113/200\n5128/5128 [==============================] - 51s 10ms/sample - loss: 0.0146 - accuracy: 0.9953 - val_loss: 0.3790 - val_accuracy: 0.9272\nEpoch 114/200\n5128/5128 [==============================] - 51s 10ms/sample - loss: 0.0136 - accuracy: 0.9958 - val_loss: 0.4317 - val_accuracy: 0.9255\nEpoch 115/200\n5128/5128 [==============================] - 51s 10ms/sample - loss: 0.0124 - accuracy: 0.9964 - val_loss: 0.3664 - val_accuracy: 0.9328\nEpoch 116/200\n5128/5128 [==============================] - 51s 10ms/sample - loss: 0.0160 - accuracy: 0.9956 - val_loss: 0.3995 - val_accuracy: 0.9226\nEpoch 117/200\n5128/5128 [==============================] - 51s 10ms/sample - loss: 0.0153 - accuracy: 0.9954 - val_loss: 0.3696 - val_accuracy: 0.9312\nEpoch 118/200\n5128/5128 [==============================] - 51s 10ms/sample - loss: 0.0150 - accuracy: 0.9953 - val_loss: 0.3134 - val_accuracy: 0.9317\nEpoch 119/200\n5128/5128 [==============================] - 51s 10ms/sample - loss: 0.0120 - accuracy: 0.9962 - val_loss: 0.3300 - val_accuracy: 0.9327\nEpoch 120/200\n5128/5128 [==============================] - 51s 10ms/sample - loss: 0.0128 - accuracy: 0.9961 - val_loss: 0.3993 - val_accuracy: 0.9272\nEpoch 121/200\n5128/5128 [==============================] - 51s 10ms/sample - loss: 0.0154 - accuracy: 0.9954 - val_loss: 0.3828 - val_accuracy: 0.9304\nEpoch 122/200\n5128/5128 [==============================] - 51s 10ms/sample - loss: 0.0166 - accuracy: 0.9949 - val_loss: 0.3432 - val_accuracy: 0.9332\nEpoch 123/200\n5128/5128 [==============================] - 51s 10ms/sample - loss: 0.0140 - accuracy: 0.9953 - val_loss: 0.3336 - val_accuracy: 0.9313\nEpoch 124/200\n5128/5128 [==============================] - 51s 10ms/sample - loss: 0.0150 - accuracy: 0.9953 - val_loss: 0.3398 - val_accuracy: 0.9326\nEpoch 125/200\n5128/5128 [==============================] - 51s 10ms/sample - loss: 0.0138 - accuracy: 0.9959 - val_loss: 0.3423 - val_accuracy: 0.9360\nEpoch 126/200\n5128/5128 [==============================] - 51s 10ms/sample - loss: 0.0145 - accuracy: 0.9956 - val_loss: 0.3815 - val_accuracy: 0.9333\nEpoch 127/200\n5128/5128 [==============================] - 51s 10ms/sample - loss: 0.0177 - accuracy: 0.9944 - val_loss: 0.3610 - val_accuracy: 0.9316\nEpoch 128/200\n5128/5128 [==============================] - 51s 10ms/sample - loss: 0.0149 - accuracy: 0.9955 - val_loss: 0.3638 - val_accuracy: 0.9248\nEpoch 129/200\n5128/5128 [==============================] - 52s 10ms/sample - loss: 0.0113 - accuracy: 0.9963 - val_loss: 0.3440 - val_accuracy: 0.9344\nEpoch 130/200\n5128/5128 [==============================] - 52s 10ms/sample - loss: 0.0146 - accuracy: 0.9954 - val_loss: 0.3305 - val_accuracy: 0.9322\nEpoch 131/200\n5128/5128 [==============================] - 52s 10ms/sample - loss: 0.0143 - accuracy: 0.9956 - val_loss: 0.5225 - val_accuracy: 0.9228\nEpoch 132/200\n5128/5128 [==============================] - 52s 10ms/sample - loss: 0.0171 - accuracy: 0.9949 - val_loss: 0.3486 - val_accuracy: 0.9314\nEpoch 133/200\n5128/5128 [==============================] - 52s 10ms/sample - loss: 0.0137 - accuracy: 0.9954 - val_loss: 0.3940 - val_accuracy: 0.9318\nEpoch 134/200\n5128/5128 [==============================] - 51s 10ms/sample - loss: 0.0145 - accuracy: 0.9960 - val_loss: 0.3320 - val_accuracy: 0.9342\nEpoch 135/200\n5128/5128 [==============================] - 51s 10ms/sample - loss: 0.0159 - accuracy: 0.9954 - val_loss: 0.4000 - val_accuracy: 0.9299\nEpoch 136/200\n5128/5128 [==============================] - 51s 10ms/sample - loss: 0.0143 - accuracy: 0.9956 - val_loss: 0.3269 - val_accuracy: 0.9360\nEpoch 137/200\n5128/5128 [==============================] - 51s 10ms/sample - loss: 0.0134 - accuracy: 0.9960 - val_loss: 0.3486 - val_accuracy: 0.9295\nEpoch 138/200\n5128/5128 [==============================] - 51s 10ms/sample - loss: 0.0162 - accuracy: 0.9951 - val_loss: 0.3315 - val_accuracy: 0.9283\nEpoch 139/200\n5128/5128 [==============================] - 51s 10ms/sample - loss: 0.0131 - accuracy: 0.9960 - val_loss: 0.3624 - val_accuracy: 0.9299\nEpoch 140/200\n5128/5128 [==============================] - 52s 10ms/sample - loss: 0.0146 - accuracy: 0.9954 - val_loss: 0.3209 - val_accuracy: 0.9314\nEpoch 141/200\n5128/5128 [==============================] - 52s 10ms/sample - loss: 0.0093 - accuracy: 0.9969 - val_loss: 0.3448 - val_accuracy: 0.9313\nEpoch 142/200\n5128/5128 [==============================] - 52s 10ms/sample - loss: 0.0129 - accuracy: 0.9959 - val_loss: 0.3373 - val_accuracy: 0.9323\nEpoch 143/200\n5128/5128 [==============================] - 52s 10ms/sample - loss: 0.0109 - accuracy: 0.9968 - val_loss: 0.3981 - val_accuracy: 0.9285\nEpoch 144/200\n5128/5128 [==============================] - 52s 10ms/sample - loss: 0.0145 - accuracy: 0.9959 - val_loss: 0.4160 - val_accuracy: 0.9305\nEpoch 145/200\n5128/5128 [==============================] - 52s 10ms/sample - loss: 0.0138 - accuracy: 0.9961 - val_loss: 0.3777 - val_accuracy: 0.9312\nEpoch 146/200\n5128/5128 [==============================] - 52s 10ms/sample - loss: 0.0139 - accuracy: 0.9959 - val_loss: 0.3791 - val_accuracy: 0.9287\nEpoch 147/200\n5128/5128 [==============================] - 52s 10ms/sample - loss: 0.0098 - accuracy: 0.9971 - val_loss: 0.4108 - val_accuracy: 0.9291\nEpoch 148/200\n5128/5128 [==============================] - 52s 10ms/sample - loss: 0.0174 - accuracy: 0.9949 - val_loss: 0.3171 - val_accuracy: 0.9339\nEpoch 149/200\n5128/5128 [==============================] - 52s 10ms/sample - loss: 0.0105 - accuracy: 0.9972 - val_loss: 0.3374 - val_accuracy: 0.9320\nEpoch 150/200\n5128/5128 [==============================] - 52s 10ms/sample - loss: 0.0107 - accuracy: 0.9964 - val_loss: 0.3635 - val_accuracy: 0.9286\nEpoch 151/200\n5128/5128 [==============================] - 52s 10ms/sample - loss: 0.0113 - accuracy: 0.9965 - val_loss: 0.4337 - val_accuracy: 0.9248\nEpoch 152/200\n5128/5128 [==============================] - 52s 10ms/sample - loss: 0.0116 - accuracy: 0.9962 - val_loss: 0.3438 - val_accuracy: 0.9299\nEpoch 153/200\n5128/5128 [==============================] - 52s 10ms/sample - loss: 0.0107 - accuracy: 0.9968 - val_loss: 0.3612 - val_accuracy: 0.9326\nEpoch 154/200\n5128/5128 [==============================] - 52s 10ms/sample - loss: 0.0095 - accuracy: 0.9973 - val_loss: 0.3677 - val_accuracy: 0.9272\nEpoch 155/200\n5128/5128 [==============================] - 52s 10ms/sample - loss: 0.0120 - accuracy: 0.9965 - val_loss: 0.3928 - val_accuracy: 0.9285\nEpoch 156/200\n5128/5128 [==============================] - 52s 10ms/sample - loss: 0.0111 - accuracy: 0.9967 - val_loss: 0.3404 - val_accuracy: 0.9320\nEpoch 157/200\n5128/5128 [==============================] - 52s 10ms/sample - loss: 0.0120 - accuracy: 0.9965 - val_loss: 0.3281 - val_accuracy: 0.9348\nEpoch 158/200\n5128/5128 [==============================] - 52s 10ms/sample - loss: 0.0131 - accuracy: 0.9960 - val_loss: 0.3548 - val_accuracy: 0.9292\nEpoch 159/200\n5128/5128 [==============================] - 52s 10ms/sample - loss: 0.0090 - accuracy: 0.9976 - val_loss: 0.3660 - val_accuracy: 0.9342\nEpoch 160/200\n5128/5128 [==============================] - 52s 10ms/sample - loss: 0.0099 - accuracy: 0.9971 - val_loss: 0.4180 - val_accuracy: 0.9261\nEpoch 161/200\n5128/5128 [==============================] - 52s 10ms/sample - loss: 0.0102 - accuracy: 0.9969 - val_loss: 0.4569 - val_accuracy: 0.9225\nEpoch 162/200\n5128/5128 [==============================] - 52s 10ms/sample - loss: 0.0148 - accuracy: 0.9958 - val_loss: 0.3401 - val_accuracy: 0.9349\nEpoch 163/200\n5128/5128 [==============================] - 52s 10ms/sample - loss: 0.0116 - accuracy: 0.9963 - val_loss: 0.4080 - val_accuracy: 0.9260\nEpoch 164/200\n5128/5128 [==============================] - 52s 10ms/sample - loss: 0.0093 - accuracy: 0.9973 - val_loss: 0.3231 - val_accuracy: 0.9339\nEpoch 165/200\n5128/5128 [==============================] - 52s 10ms/sample - loss: 0.0108 - accuracy: 0.9969 - val_loss: 0.4254 - val_accuracy: 0.9236\nEpoch 166/200\n5128/5128 [==============================] - 52s 10ms/sample - loss: 0.0147 - accuracy: 0.9960 - val_loss: 0.3613 - val_accuracy: 0.9275\nEpoch 167/200\n5128/5128 [==============================] - 52s 10ms/sample - loss: 0.0124 - accuracy: 0.9966 - val_loss: 0.3203 - val_accuracy: 0.9359\nEpoch 168/200\n5128/5128 [==============================] - 52s 10ms/sample - loss: 0.0082 - accuracy: 0.9973 - val_loss: 0.3227 - val_accuracy: 0.9354\nEpoch 169/200\n5128/5128 [==============================] - 52s 10ms/sample - loss: 0.0082 - accuracy: 0.9974 - val_loss: 0.3452 - val_accuracy: 0.9343\nEpoch 170/200\n5128/5128 [==============================] - 52s 10ms/sample - loss: 0.0128 - accuracy: 0.9964 - val_loss: 0.3565 - val_accuracy: 0.9317\nEpoch 171/200\n5128/5128 [==============================] - 52s 10ms/sample - loss: 0.0094 - accuracy: 0.9974 - val_loss: 0.3275 - val_accuracy: 0.9308\nEpoch 172/200\n5128/5128 [==============================] - 52s 10ms/sample - loss: 0.0096 - accuracy: 0.9973 - val_loss: 0.3586 - val_accuracy: 0.9298\nEpoch 173/200\n5128/5128 [==============================] - 52s 10ms/sample - loss: 0.0125 - accuracy: 0.9965 - val_loss: 0.4072 - val_accuracy: 0.9287\nEpoch 174/200\n5128/5128 [==============================] - 52s 10ms/sample - loss: 0.0118 - accuracy: 0.9969 - val_loss: 0.3321 - val_accuracy: 0.9379\nEpoch 175/200\n5128/5128 [==============================] - 52s 10ms/sample - loss: 0.0105 - accuracy: 0.9969 - val_loss: 0.3265 - val_accuracy: 0.9350\nEpoch 176/200\n5128/5128 [==============================] - 52s 10ms/sample - loss: 0.0106 - accuracy: 0.9970 - val_loss: 0.3336 - val_accuracy: 0.9335\nEpoch 177/200\n5128/5128 [==============================] - 52s 10ms/sample - loss: 0.0127 - accuracy: 0.9967 - val_loss: 0.3477 - val_accuracy: 0.9353\nEpoch 178/200\n5128/5128 [==============================] - 52s 10ms/sample - loss: 0.0106 - accuracy: 0.9968 - val_loss: 0.3488 - val_accuracy: 0.9269\nEpoch 179/200\n5128/5128 [==============================] - 52s 10ms/sample - loss: 0.0103 - accuracy: 0.9969 - val_loss: 0.3997 - val_accuracy: 0.9328\nEpoch 180/200\n5128/5128 [==============================] - 52s 10ms/sample - loss: 0.0129 - accuracy: 0.9964 - val_loss: 0.3887 - val_accuracy: 0.9311\nEpoch 181/200\n5128/5128 [==============================] - 52s 10ms/sample - loss: 0.0096 - accuracy: 0.9971 - val_loss: 0.3597 - val_accuracy: 0.9281\nEpoch 182/200\n5128/5128 [==============================] - 50s 10ms/sample - loss: 0.0094 - accuracy: 0.9974 - val_loss: 0.3728 - val_accuracy: 0.9297\nEpoch 183/200\n5128/5128 [==============================] - 49s 10ms/sample - loss: 0.0114 - accuracy: 0.9968 - val_loss: 0.3408 - val_accuracy: 0.9346\nEpoch 184/200\n5128/5128 [==============================] - 49s 10ms/sample - loss: 0.0090 - accuracy: 0.9970 - val_loss: 0.3160 - val_accuracy: 0.9347\nEpoch 185/200\n5128/5128 [==============================] - 49s 10ms/sample - loss: 0.0092 - accuracy: 0.9973 - val_loss: 0.3639 - val_accuracy: 0.9277\nEpoch 186/200\n5128/5128 [==============================] - 49s 10ms/sample - loss: 0.0113 - accuracy: 0.9966 - val_loss: 0.4088 - val_accuracy: 0.9297\nEpoch 187/200\n5128/5128 [==============================] - 49s 10ms/sample - loss: 0.0105 - accuracy: 0.9968 - val_loss: 0.3459 - val_accuracy: 0.9347\nEpoch 188/200\n5128/5128 [==============================] - 49s 10ms/sample - loss: 0.0083 - accuracy: 0.9973 - val_loss: 0.3485 - val_accuracy: 0.9388\nEpoch 189/200\n5128/5128 [==============================] - 49s 10ms/sample - loss: 0.0117 - accuracy: 0.9968 - val_loss: 0.4207 - val_accuracy: 0.9230\nEpoch 190/200\n5128/5128 [==============================] - 49s 10ms/sample - loss: 0.0124 - accuracy: 0.9966 - val_loss: 0.3594 - val_accuracy: 0.9336\nEpoch 191/200\n5128/5128 [==============================] - 49s 10ms/sample - loss: 0.0094 - accuracy: 0.9976 - val_loss: 0.3414 - val_accuracy: 0.9392\nEpoch 192/200\n5128/5128 [==============================] - 49s 10ms/sample - loss: 0.0132 - accuracy: 0.9961 - val_loss: 0.3724 - val_accuracy: 0.9320\nEpoch 193/200\n5128/5128 [==============================] - 49s 10ms/sample - loss: 0.0077 - accuracy: 0.9978 - val_loss: 0.3599 - val_accuracy: 0.9321\nEpoch 194/200\n5128/5128 [==============================] - 49s 10ms/sample - loss: 0.0110 - accuracy: 0.9968 - val_loss: 0.3481 - val_accuracy: 0.9345\nEpoch 195/200\n5128/5128 [==============================] - 49s 10ms/sample - loss: 0.0093 - accuracy: 0.9974 - val_loss: 0.3045 - val_accuracy: 0.9367\nEpoch 196/200\n5128/5128 [==============================] - 49s 10ms/sample - loss: 0.0112 - accuracy: 0.9970 - val_loss: 0.4090 - val_accuracy: 0.9324\nEpoch 197/200\n5128/5128 [==============================] - 49s 10ms/sample - loss: 0.0109 - accuracy: 0.9967 - val_loss: 0.3493 - val_accuracy: 0.9320\nEpoch 198/200\n5128/5128 [==============================] - 49s 10ms/sample - loss: 0.0091 - accuracy: 0.9974 - val_loss: 0.3716 - val_accuracy: 0.9332\nEpoch 199/200\n5128/5128 [==============================] - 49s 10ms/sample - loss: 0.0126 - accuracy: 0.9967 - val_loss: 0.3162 - val_accuracy: 0.9345\nEpoch 200/200\n5128/5128 [==============================] - 49s 10ms/sample - loss: 0.0081 - accuracy: 0.9975 - val_loss: 0.3893 - val_accuracy: 0.9314\n"
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, validation_data=(X_validation, y_validation), batch_size=128, epochs=200,callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model441x2.h5')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37764bitthivmconda23b37608f7144fb0aec54a3bface4bbd",
   "display_name": "Python 3.7.7 64-bit ('thivm': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}